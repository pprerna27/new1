import pandas as pd
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# Sample DataFrame - Replace this with your actual data
data = {
    "text": [
        "Regulation A requires X",
        "Regulation B prohibits Y",
        "Regulation C allows Z",
        "Regulation D mandates W"
    ],
    "obligation": ["Yes", "No", "No", "Yes"]
}
df = pd.DataFrame(data)

# Splitting the data into training and testing sets
X = df['text']  # Text column
y = df['obligation']  # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preparing data for Doc2Vec
train_documents = [TaggedDocument(words=sentence.split(), tags=[str(i)]) for i, sentence in enumerate(X_train)]

# Training the Doc2Vec model
doc2vec_model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=20)
doc2vec_model.build_vocab(train_documents)
doc2vec_model.train(train_documents, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)

# Function to get Doc2Vec embeddings for a text
def get_doc2vec_vector(text, model):
    words = text.split()
    return model.infer_vector(words)

# Converting texts to vectors
X_train_vectors = np.array([get_doc2vec_vector(sentence, doc2vec_model) for sentence in X_train])
X_test_vectors = np.array([get_doc2vec_vector(sentence, doc2vec_model) for sentence in X_test])

# Building the Logistic Regression model
model = LogisticRegression()
model.fit(X_train_vectors, y_train)

# Making predictions
y_pred = model.predict(X_test_vectors)

# Evaluating the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Example of predicting new text
new_text = ["Regulation E introduces Q"]
new_text_vectorized = np.array([get_doc2vec_vector(sentence, doc2vec_model) for sentence in new_text])
prediction = model.predict(new_text_vectorized)
print("\nPrediction for new text:", prediction[0])

import pandas as pd
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import numpy as np

# Example DataFrame
data = {'text_column': ['This is a sample sentence.', 
                        'Tokenize this text for vectorization.', 
                        'Count and TF-IDF are useful.']}
df = pd.DataFrame(data)

# Tag documents
tagged_documents = [TaggedDocument(words=text.split(), tags=[i]) for i, text in enumerate(df['text_column'])]

# Train Doc2Vec model
doc2vec_model = Doc2Vec(tagged_documents, vector_size=100, window=5, min_count=1, workers=4)

# Get document embeddings
df['doc_embedding'] = [doc2vec_model.dv[i] for i in range(len(df))]

# Convert embeddings to feature matrix
X = np.vstack(df['doc_embedding'].values)
y = [1, 0, 1]  # Replace with real labels

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train logistic regression model
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

# Evaluate
accuracy = log_reg.score(X_test, y_test)
print(f"Model Accuracy (Doc2Vec): {accuracy:.2f}")
