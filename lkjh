import os
import time
import requests
import pandas as pd
import PyPDF2
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# Function to set up Selenium WebDriver
def setup_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # Run in headless mode (no browser UI)
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    return driver

# Function to extract PDF links using Selenium
def get_pdf_links(url):
    driver = setup_driver()
    driver.get(url)
    time.sleep(3)  # Wait for page to load

    pdf_links = []
    elements = driver.find_elements(By.TAG_NAME, "a")  # Find all links
    for element in elements:
        href = element.get_attribute("href")
        if href and href.endswith('.pdf'):  # Filter only PDF links
            pdf_links.append(href)

    driver.quit()
    return pdf_links

# Function to download a PDF and count its pages
def download_and_count_pages(pdf_url, save_folder):
    pdf_name = pdf_url.split('/')[-1]
    pdf_path = os.path.join(save_folder, pdf_name)

    # Download the PDF
    response = requests.get(pdf_url)
    with open(pdf_path, 'wb') as f:
        f.write(response.content)

    # Count pages
    try:
        with open(pdf_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            num_pages = len(reader.pages)
    except Exception as e:
        num_pages = None  # Handle errors gracefully

    return pdf_name, num_pages

# Main function
def scrape_pdfs(url, save_folder='pdf_downloads'):
    os.makedirs(save_folder, exist_ok=True)  # Create folder if not exists

    pdf_links = get_pdf_links(url)
    pdf_data = []

    for pdf_url in pdf_links:
        pdf_name, num_pages = download_and_count_pages(pdf_url, save_folder)
        pdf_data.append({'PDF Name': pdf_name, 'Number of Pages': num_pages})

    # Convert to DataFrame and export
    df = pd.DataFrame(pdf_data)
    df.to_csv('pdf_report.csv', index=False)

    return df

# Example usage
url = "https://example.com"  # Replace with the actual webpage containing PDFs
df = scrape_pdfs(url)
print(df)
